import os
import re
import time
import requests
import json
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup

# --- C·∫§U H√åNH ---
BASE_OUTPUT_DIR = "data/CongThongTinDienTu/ChiThi"
os.makedirs(BASE_OUTPUT_DIR, exist_ok=True)

METADATA_FILE = os.path.join(BASE_OUTPUT_DIR, "metadata_congthongtindientu_chithi.jsonl")
OS_CHARS_INVALID = r'[<>:"/\\|?*]'  # C√°c k√Ω t·ª± c·∫•m trong t√™n file Windows


def setup_driver():
    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--window-size=1920,1080")
    # Th√™m user-agent ƒë·ªÉ tr√°nh b·ªã ch·∫∑n
    chrome_options.add_argument(
        "user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36")
    service = Service(ChromeDriverManager().install())
    return webdriver.Chrome(service=service, options=chrome_options)


def sanitize_filename(name):
    """L√†m s·∫°ch t√™n file ƒë·ªÉ l∆∞u ƒë∆∞·ª£c tr√™n ·ªï c·ª©ng"""
    name = re.sub(OS_CHARS_INVALID, "", name)
    name = re.sub(r"\s+", "_", name)  # Thay kho·∫£ng tr·∫Øng b·∫±ng _
    return name.strip()[:200]  # C·∫Øt ng·∫Øn n·∫øu qu√° d√†i


def download_pdf(pdf_url, save_path):
    if os.path.exists(save_path):
        print(f"‚ö†Ô∏è File ƒë√£ t·ªìn t·∫°i: {save_path}")
        return "ƒê√£ t·ªìn t·∫°i"

    try:
        headers = {"User-Agent": "Mozilla/5.0"}  # Fake header request
        response = requests.get(pdf_url, headers=headers, stream=True, timeout=30)
        response.raise_for_status()
        with open(save_path, "wb") as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
        print(f"‚úÖ T·∫£i th√†nh c√¥ng: {os.path.basename(save_path)}")
        return "T·∫£i th√†nh c√¥ng"
    except Exception as e:
        print(f"‚ùå L·ªói t·∫£i file: {e}")
        return f"L·ªói: {str(e)}"


def append_metadata(metadata):
    with open(METADATA_FILE, "a", encoding='utf-8') as f:
        json.dump(metadata, f, ensure_ascii=False)
        f.write("\n")


def load_existing_urls():
    """D√πng URL b√†i vi·∫øt l√†m key check tr√πng"""
    existing_urls = set()
    if os.path.exists(METADATA_FILE):
        with open(METADATA_FILE, "r", encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    try:
                        data = json.loads(line.strip())
                        # L∆∞u URL b√†i vi·∫øt g·ªëc ƒë·ªÉ check
                        existing_urls.add(data.get("Ngu·ªìn"))
                    except:
                        pass
    return existing_urls


def get_ngay(text):
    """Tr√≠ch xu·∫•t ng√†y th√°ng nƒÉm"""
    # T√¨m chu·ªói d·∫°ng dd/mm/yyyy
    match = re.search(r"\d{1,2}[/-]\d{1,2}[/-]\d{4}", text)
    if match:
        return match.group(0)
    return ""


def main():
    driver = setup_driver()
    try:
        existing_urls = load_existing_urls()
        print(f"ƒê√£ load {len(existing_urls)} b√†i vi·∫øt ƒë√£ c√†o tr∆∞·ªõc ƒë√≥.")

        page_start = 5
        page_end = 8  # Data t·ª´ 2020
        for page in range(page_start, page_end + 1):
            print(f"\n>>> üìÑ ƒêANG C√ÄO TRANG: {page}")

            # URL t√¨m ki·∫øm
            url_list = (f"https://congbao.chinhphu.vn/tim-kiem-van-ban?_csrf=WERTZjNNTmINEwoDXwZ9GBUbJjlZBSsRHiExBH8OeCQZNDoyXzktEw%3D%3D&trichyeu="
                        f"&coquanbanhanh=&tungay=01%2F01%2F2020&denngay=&sovanban=&loaivanban=4&nguoiky=&page={page}")

            driver.get(url_list)
            time.sleep(2)  # Ch·ªù JS load list

            soup_list = BeautifulSoup(driver.page_source, "html.parser")
            articles = soup_list.find_all("article", class_="cong-bao-list")

            if not articles:
                print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y b√†i vi·∫øt n√†o. C√≥ th·ªÉ h·∫øt trang ho·∫∑c b·ªã ch·∫∑n.")
                break

            for item in articles:
                try:
                    # 1. L·∫•y th√¥ng tin c∆° b·∫£n
                    header_tag = item.find("header").find("h1").find("a")
                    ten_van_ban = re.sub(r"\s+", " ", header_tag.get_text(strip=True))
                    real_link = "https://congbao.chinhphu.vn" + header_tag["href"]

                    # CHECK TR√ôNG: N·∫øu link ƒë√£ c√†o r·ªìi th√¨ b·ªè qua ngay
                    if real_link in existing_urls:
                        print(f"‚è≠Ô∏è B·ªè qua (ƒë√£ c√†o): {ten_van_ban[:50]}...")
                        continue

                    print(f"üîç ƒêang x·ª≠ l√Ω: {ten_van_ban}")

                    # L·∫•y c√°c th√¥ng tin kh√°c
                    so_hieu = "Kh√¥ng c√≥ s·ªë hi·ªáu"
                    match_so = re.search(r"S·ªë:\s*([^\s]+)", ten_van_ban)  # Regex t√¨m s·ªë hi·ªáu t·ªët h∆°n
                    if match_so: so_hieu = match_so.group(1)

                    section_p = item.find("section").find("p")
                    summary = section_p.get_text(strip=True) if section_p else ""

                    # L·∫•y ng√†y ban h√†nh / hi·ªáu l·ª±c
                    ngay_ban_hanh = ""
                    ngay_hieu_luc = ""
                    footer = item.find("footer")
                    if footer:
                        spans = footer.find_all("span")
                        for span in spans:
                            txt = span.get_text(strip=True).lower()
                            if "ban h√†nh" in txt:
                                ngay_ban_hanh = get_ngay(txt)
                            elif "hi·ªáu l·ª±c" in txt:
                                ngay_hieu_luc = get_ngay(txt)

                    # 2. V√†o trang chi ti·∫øt ƒë·ªÉ l·∫•y PDF
                    driver.get(real_link)
                    time.sleep(1.5)  # Ch·ªù load trang con

                    soup_detail = BeautifulSoup(driver.page_source, "html.parser")

                    # T√¨m link PDF
                    pdf_url = None
                    pdf_name_raw = f"ChiThi_{so_hieu.replace('/', '-')}.pdf"  # Default name

                    # C√°ch t√¨m link PDF c·ªßa b·∫°n (Dropdown)
                    menu = soup_detail.find("ul", class_="dropdown-menu")
                    if menu:
                        link_tags = menu.find_all("a")
                        for a in link_tags:
                            href = a.get("href", "")
                            if "format=pdf" in href:
                                pdf_url = f"https://congbao.chinhphu.vn{href}"
                                pdf_name_raw = a.get_text(strip=True) or pdf_name_raw
                                break

                    status_tai = "Kh√¥ng t√¨m th·∫•y link PDF"
                    final_path = ""

                    if pdf_url:
                        # T·∫°o t√™n file an to√†n
                        safe_name = sanitize_filename(pdf_name_raw)
                        if not safe_name.lower().endswith(".pdf"): safe_name += ".pdf"

                        final_path = os.path.join(BASE_OUTPUT_DIR, safe_name)
                        status_tai = download_pdf(pdf_url, final_path)
                    else:
                        print("‚ö†Ô∏è Kh√¥ng th·∫•y n√∫t t·∫£i PDF.")

                    # 3. L∆∞u Metadata
                    metadata = {
                        "T√™n file": final_path,
                        "T√™n vƒÉn b·∫£n": ten_van_ban,
                        "Tr√≠ch y·∫øu": summary,
                        "S·ªë hi·ªáu": so_hieu,
                        "Ng√†y ban h√†nh": ngay_ban_hanh,
                        "Ng√†y hi·ªáu l·ª±c": ngay_hieu_luc,
                        "Lo·∫°i vƒÉn b·∫£n": "Ch·ªâ th·ªã",
                        "Ngu·ªìn": real_link,
                        "PDF URL": pdf_url or "",
                        "Tr·∫°ng th√°i t·∫£i": status_tai
                    }

                    append_metadata(metadata)
                    existing_urls.add(real_link)  # C·∫≠p nh·∫≠t set ƒë·ªÉ loop sau kh√¥ng b·ªã tr√πng

                    # Ngh·ªâ nh·∫π ƒë·ªÉ tr√°nh DDOS server h·ªç
                    time.sleep(1)

                except Exception as e:
                    print(f"‚ùå L·ªói x·ª≠ l√Ω item: {e}")
                    continue

    finally:
        driver.quit()
        print("üëã ƒê√£ ƒë√≥ng tr√¨nh duy·ªát.")


if __name__ == "__main__":
    main() # Mai check trang C√¥ng b√°o ƒë·ªÉ thu th·∫≠p to√†n b·ªô file data + update metadata jsonl + ƒëi·ªÅu ch·ªânh c√°c file li√™n quan (update profile c·ªßa chatbot)